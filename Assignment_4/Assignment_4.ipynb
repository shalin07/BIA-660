{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of Question 1:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Grant Cornwell', ' College of Wooster', '2015', '911,651'),\n",
       " ('Marvin Krislov', ' Oberlin College', '2016', '829,913'),\n",
       " ('Mark Roosevelt', ' Antioch College', '2015', '507,672'),\n",
       " ('Laurie Joyner', ' Wittenberg University', '2015', '463,504'),\n",
       " ('Richard Giese', ' University of Mount Union', '2015', '453,800')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Q1\n",
      "\n",
      "lemmatized: False, no_stopword: False\n",
      "0.6304347826086957\n",
      "\n",
      "Test Q2\n",
      "\n",
      "lemmatized: True, no_stopword: False\n",
      "0.782608695652174\n",
      "\n",
      "Test Q3\n",
      "\n",
      "lemmatized: False, no_stopword: True\n",
      "0.6358695652173914\n",
      "\n",
      "Test Q4\n",
      "\n",
      "lemmatized: True, no_stopword: True\n",
      "0.7717391304347826\n",
      "\n",
      "Output of Q3\n",
      "0.7717391304347826 0.4982456140350877\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import re\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance\n",
    "from termcolor import colored\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm \n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def extract(text):\n",
    "    return re.findall('([A-Z]{1}[a-z]+ [A-Z]{1}[a-z]+),([ a-zA-Z]+[a-z]{1})[ ,(a-z]+(2[0-9]{3})[): $]+([0-9]{3},[0-9]{3})', text)\n",
    "text='''Following is total compensation for other presidents at privat\n",
    "e colleges in Ohio in 2015:\n",
    "Grant Cornwell, College of Wooster (left in 2015): $911,651\n",
    "Marvin Krislov, Oberlin College (left in 2016): $829,913\n",
    "Mark Roosevelt, Antioch College, (left in 2015): $507,672\n",
    "Laurie Joyner, Wittenberg University (left in 2015): $463,504\n",
    "Richard Giese, University of Mount Union (left in 2015): $453,800'''\n",
    "\n",
    "def tokenize(text, lemmatized = False, no_stopword = False):\n",
    "    tokens = []\n",
    "    text = nlp(text)\n",
    "    if lemmatized:\n",
    "        if no_stopword:\n",
    "            tokens = [token.lemma_ for token in text if not token.is_stop]\n",
    "        else:\n",
    "            tokens = [token.lemma_ for token in text]\n",
    "    if lemmatized == False:\n",
    "        if no_stopword:\n",
    "            tokens = [token.text for token in text if not token.is_stop]\n",
    "        else:\n",
    "            tokens = [token.text for token in text]\n",
    "    token_count = FreqDist(tokens)\n",
    "    return token_count\n",
    "\n",
    "def get_similarity(q1, q2, lemmatized=False, no_stopword=False):\n",
    "    a = lemmatized\n",
    "    b = no_stopword\n",
    "    sim = []\n",
    "    q1 = q1 + q2\n",
    "    for i in q1:\n",
    "        q3 = [''.join(i) for i in q1]\n",
    "    docs_tokens={idx:tokenize(doc,a,b) for idx,doc in enumerate(q3)}\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(q3)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    similarity=1-distance.squareform (distance.pdist(smoothed_tf_idf, 'cosine'))\n",
    "    for i in range(0,500):\n",
    "        sim.append(similarity[i,i+500])\n",
    "    return sim\n",
    "\n",
    "data = pd.read_csv(\"quora_duplicate_question_500.csv\")\n",
    "q1 = data[\"q1\"].values.tolist()\n",
    "q2 = data[\"q2\"].values.tolist()\n",
    "ground_truth = data[\"is_duplicate\"].values\n",
    "def predict(sim, ground_truth, threshold = 0.5):\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range(0,500):\n",
    "        if sim[i] > threshold:\n",
    "            sim[i] = 1\n",
    "        else:\n",
    "            sim[i] = 0\n",
    "    pred = sim\n",
    "    for i in range(500):\n",
    "        if sim[i] == ground_truth[i] == 1:\n",
    "            count += 1\n",
    "    for i in range(500):\n",
    "        if sim[i]==1:\n",
    "            count2 += 1\n",
    "    count1=data['q1'].loc[(data.is_duplicate == 1.0)].count()\n",
    "    recall = count/count1\n",
    "    \n",
    "    return pred, recall\n",
    "\n",
    "def evaluate(sim, ground_truth, threshold = 0.5 ):\n",
    "    count = 0\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    for i in range(0,500):\n",
    "        if sim[i] > threshold:\n",
    "            sim[i] = 1\n",
    "        else:\n",
    "            sim[i] = 0\n",
    "    for i in range(500):\n",
    "        if sim[i] == ground_truth[i] == 1:\n",
    "            count += 1\n",
    "    for i in range(500):\n",
    "        if sim[i]==1:\n",
    "            count2 += 1\n",
    "    count1=data['q1'].loc[(data.is_duplicate == 1.0)].count()\n",
    "    recall = count/count1\n",
    "    precision = count/count2\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "\n",
    "print(colored(\"Output of Question 1:\", \"blue\", attrs=['bold']))\n",
    "extract(text)\n",
    "\n",
    "print(colored(\"Test Q1\", \"green\", attrs=['bold']))\n",
    "print(\"\\nlemmatized: False, no_stopword: False\")\n",
    "sim = get_similarity(q1, q2, lemmatized=False, no_stopword=False)\n",
    "pred, recall = predict(sim, ground_truth)\n",
    "print(recall)\n",
    "print('')\n",
    "\n",
    "\n",
    "print(colored(\"Test Q2\", \"green\", attrs=['bold']))\n",
    "print(\"\\nlemmatized: True, no_stopword: False\")\n",
    "sim = get_similarity(q1, q2, lemmatized=True, no_stopword=False)\n",
    "pred, recall = predict(sim, ground_truth)\n",
    "print(recall)\n",
    "print('')\n",
    "\n",
    "\n",
    "print(colored(\"Test Q3\", \"green\", attrs=['bold']))\n",
    "print(\"\\nlemmatized: False, no_stopword: True\")\n",
    "sim = get_similarity(q1, q2, lemmatized=False, no_stopword=True)\n",
    "pred, recall = predict(sim, ground_truth)\n",
    "print(recall)\n",
    "print('')\n",
    "\n",
    "print(colored(\"Test Q4\", \"green\", attrs=['bold']))\n",
    "print(\"\\nlemmatized: True, no_stopword: True\")\n",
    "sim = get_similarity(q1, q2, lemmatized=True, no_stopword=True)\n",
    "pred, recall = predict(sim, ground_truth)\n",
    "print(recall)\n",
    "print('')\n",
    "\n",
    "print(colored(\"Output of Q3\", \"green\", attrs=['bold']))\n",
    "recall, precision = evaluate(sim, ground_truth, threshold = 0.5 )\n",
    "print(recall, precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
